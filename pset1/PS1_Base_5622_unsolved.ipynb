{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1 - K-Nearest Neighbor and NaiveBayes classifiers\n",
    "## CSCI 5622 - Spring 2022\n",
    "\n",
    "\n",
    "Student name: Aditya Srivastava\n",
    "\n",
    "Only submit this notebook to canvas (no zip files).\n",
    "\n",
    "For today's assignment, we will be implementing our own K-Nearest Neighbors classifier (KNNClassifier) algorithm and a Naive Bayes classifier. \n",
    "\n",
    "*But Professor Quigley, hasn't someone else already written KNN before?*\n",
    "\n",
    "Yes, you are not the first to implement KNN, or basically any algorithm we'll work with in this class. But\n",
    "1. I'll know that you know what's really going on\n",
    "2. You'll know you can do it, because\n",
    "    1. someday you might have to implement some machine learning algorithm from scratch - maybe for a new platform (do you need to run python on your SmartToaster just to get it to\n",
    "learn how users like their toast?), maybe because you want to tweak the algorithm (there's always a better approach...),\n",
    "    2. maybe because you're working on something important, and you need to control exactly what's on there\n",
    "(should you really be running anaconda on your secret spy plane?).\n",
    "\n",
    "That said - we're not going to implement *everything*. We'll start by importing a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.neighbors\n",
    "import data\n",
    "import tests\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "588da94aa7fe878809adb07fe913cd1d",
     "grade": false,
     "grade_id": "descrip",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Wait a minute - didn't we just import Scikit-learn (sklearn)? The package with baked-in machine learning tools?*\n",
    " Yes - but it also has a ton of helper functions that we'll be using later.\n",
    "\n",
    "You will be guided through the different questions and you'll be expected to complete the classes and the functions following the provided signatures.\n",
    " Sometimes at the end of a question we would provide a difficulty estimate using\n",
    "the average scored by students who attempted the question (or a similar one) in previous assignments.\n",
    "\n",
    "Remember to avoid adding positional arguments and make sure the returned values have the correct format (this applies to all assignments).\n",
    "The alternative is that your solution might be rejected by the auto-grader (we won't be debugging your code).\n",
    "We will provide some basic sanity checks. They're in no means exhaustive and passing them does not imply\n",
    "your solution is 100% correct.\n",
    "\n",
    "\n",
    "For example, you're required to complete the method `compute_something` of class A.\n",
    "We provide examples of acceptable solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell can be removed from the submitted notebook\n",
    "class A:\n",
    "    def compute_something(self, X):\n",
    "        \"\"\"\n",
    "        :return: numpy array of zeros, with shape (4,)\n",
    "        \"\"\"\n",
    "        # BEGIN\n",
    "        answer = None\n",
    "        # END\n",
    "        return answer\n",
    "\n",
    "\n",
    "class A1:  # Acceptable solution:\n",
    "    # - the added y is an optional argument and omitting it does not affect the solution\n",
    "    # - the returned object has the expected structure.\n",
    "    def compute_something(self, X, y=None):\n",
    "        # BEGIN\n",
    "        return np.zeros((4,))\n",
    "        # END\n",
    "\n",
    "\n",
    "class A2:  # Wrong format:\n",
    "    # - your solution requires a new positional argument y!\n",
    "    # - the returned object does not have the expected format!\n",
    "    # - solution outside the delimiters # BEGIN # END\n",
    "    def compute_something(self, X, y):\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "class A3:  # Acceptable solution:\n",
    "    # You're free to add your own helper functions/methods\n",
    "    def compute_something(self, X):\n",
    "        # BEGIN\n",
    "        return self.get_zeros(4)\n",
    "        # END\n",
    "\n",
    "    def get_zeros(self, i):\n",
    "        return np.zeros((i,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a738b5c58c23dedcfd143c28d17cfb52",
     "grade": false,
     "grade_id": "cell-295972aa87cb3512",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's also load a dataset to play with and start working to build out our own classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_data = data.BinaryData()\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(6), fig.set_figwidth(18)\n",
    "for i, name in enumerate([\"training\", \"validation\", \"test\"]):\n",
    "    axs[i].plot(*binary_data.boundary())\n",
    "    axs[i].set_title(\"%s samples\" % name)\n",
    "axs[0].scatter(binary_data.X_train[:, 0], binary_data.X_train[:, 1], c=binary_data.y_train)\n",
    "axs[1].scatter(binary_data.X_valid[:, 0], binary_data.X_valid[:, 1], c=binary_data.y_valid)\n",
    "axs[2].scatter(binary_data.X_test[:, 0], binary_data.X_test[:, 1], c=binary_data.y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8cf4b00b0ca0d7a9a03d48f513d555b",
     "grade": false,
     "grade_id": "knn_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have data! The `binary_data` instance has the following attributes:\n",
    "   - a training set (`X_train, y_train`): to train the model and on which the prediction is based\n",
    "   - a validation set (`X_valid, y_valid`): to select the best __hyper-parameters__ of the model\n",
    "   - a test set (`X_test, y_test`): to evaluate the performance of the model on unseen data\n",
    "\n",
    "### Problem 1: Complete our KNN Classifier - 30 Points\n",
    "\n",
    "The KNNClassifier class we're implementing will have similar design to the K-Nearest Neighbors classifier class from *scikit-learn*:\n",
    "- Initialize the classifier with corresponding parameters (number of neighbors k)\n",
    "- A `fit` method that uses the training data\n",
    "- A `predict` method that returns the predicted labels given data `X`\n",
    "\n",
    "We've written out a lot of the structure for you for consistency across different parts of the assignment and so\n",
    "you can focus on the \"important\" stuff that actually relates to the machine learning itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9de44f5e4632e7b19985caba0617c9d",
     "grade": true,
     "grade_id": "KNNCode",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "\n",
    "    def __init__(self, k=5):\n",
    "        \"\"\"\n",
    "        Initialize our custom KNN classifier\n",
    "        :param k: the number of nearest neighbors to consider for classification\n",
    "        \"\"\"\n",
    "        self._k = k\n",
    "        self._ball_tree = None\n",
    "        self._y = None\n",
    "        self.label_to_index = None\n",
    "        self.index_to_label = None\n",
    "        self.population_most_common = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using the provided data\n",
    "        :param X: 2-D np.array of shape (number training samples, number of features)\n",
    "        :param y: 1-D np.array of shape (number training samples,)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self._ball_tree = sklearn.neighbors.BallTree(X)  # See documentation of BallTree and how it's used\n",
    "        self._y = y\n",
    "        # Should be used to map the classes to {0,1,..C-1} if needed (C is the number of classes)\n",
    "        # We can assume that the training data contains samples from all the possible classes\n",
    "        classes = np.unique(y)\n",
    "        self.label_to_index = dict(zip(classes, range(classes.shape[0])))\n",
    "        self.index_to_label = dict(zip(range(classes.shape[0]), classes))\n",
    "\n",
    "        label_values, label_counts = np.unique(y, return_counts=True)\n",
    "        self.training_most_common = label_values[np.argmax(label_counts)]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def majority_vote(self, indices_nearest_k, distances_nearest_k=None):\n",
    "        \"\"\"\n",
    "        Given indices of the nearest k neighbors for each point, report the majority label of those points.\n",
    "        :param indices_nearest_k: np.array containing the indices of training neighbors, of shape (M, k)\n",
    "        :param distances_nearest_k: np.array containing the corresponding distances of training neighbors, of shape (M, k)\n",
    "        :return: The majority label for each row of indices, shape (M,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.1\n",
    "        # TODO: Determine majority for each row of indices_nearest_k\n",
    "        # TODO: if there is a tie, set the label to the most common label in the training set\n",
    "        #BEGIN \n",
    "        voted_labels = np.empty(indices_nearest_k.shape[0]) # You can replace this\n",
    "        # code here\n",
    "        #END\n",
    "        return voted_labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given new data points, classify them according to the training data provided in fit and number of neighbors k\n",
    "        You should use BallTree to get the distances and indices of the nearest k neighbors\n",
    "        :param X: feature vectors (num_points, num_features)\n",
    "        :return: 1-D np.array of predicted classes of shape (num_points,)\n",
    "        \"\"\"\n",
    "        # Workspace 1.2\n",
    "        #BEGIN \n",
    "        distances_nearest_k, indices_nearest_k = None, None  # REPLACE\n",
    "        # code here\n",
    "        #END\n",
    "        return self.majority_vote(indices_nearest_k, distances_nearest_k)\n",
    "\n",
    "    def confusion_matrix(self, X, y):\n",
    "        \"\"\"\n",
    "        Generate the confusion matrix for the given data\n",
    "        :param X: an np.array of feature vectors of points, shape (N, n_features)\n",
    "        :param y: the corresponding correct classes of our set, shape (N,)\n",
    "        :return: a C*C np.array of counts, where C is the number of classes in our training data\n",
    "        \"\"\"\n",
    "        # Workspace 1.3\n",
    "        # The rows of the confusion matrix correspond to the counts from the true labels, the columns to the predictions'\n",
    "        # TODO: Run classification for the test set X, compare to test answers y, and add counts to matrix\n",
    "        #BEGIN \n",
    "        c_matrix = np.zeros((len(self.label_to_index), len(self.label_to_index)))\n",
    "        # code here\n",
    "        #END\n",
    "        return c_matrix\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the accuracy of the classifier on the data (X_test, y_test)\n",
    "        :param X: np.array of shape (m, number_features)\n",
    "        :param y: np.array of shape (m,)\n",
    "        :return: accuracy score [float in (0,1)]\n",
    "        \"\"\"\n",
    "        # Workspace 1.4\n",
    "        # TODO: Compute accuracy on X\n",
    "        #BEGIN \n",
    "        score = 0 # REPLACE\n",
    "        # code here\n",
    "        #END\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.testKNN(KNNClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45ab150018c62637009e612ea4c350db",
     "grade": false,
     "grade_id": "knn_q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*But professor, this code isn't complete!*\n",
    "\n",
    "1.1 [5 points] Complete the `majority_vote` function to determine the majority class of a series of neighbors.\n",
    "If there is a tie, then you should remove the farthest element until the tie is broken. (Avg __4.6__)\n",
    "\n",
    "1.2 [5 points] Complete the `predict` function to capture the predicted class of a new datapoint (Avg __4.9__)\n",
    "\n",
    " - HINT: Use the BallTree documentation to determine how to retrieve neighbors from the model (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree)\n",
    "\n",
    "1.3 [5 points] Complete the `confusion_matrix` function to reveal the results of classification (Avg __5__)\n",
    "\n",
    "1.4 [5 points] Complete the `accuracy` function to get accuracy of the classifier based on a given test data (Avg __5__)\n",
    "\n",
    "Below, we'll be using our KNNClassifier (sent in as \"model\") to show how we would predict any points in space given the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is to show the decision surface of the classifier\n",
    "# You can change k to visualize KNN behavior\n",
    "knn = KNNClassifier(2).fit(binary_data.X_train, binary_data.y_train)\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(6), fig.set_figwidth(18)\n",
    "tests.show_decision_surface(knn, binary_data.X_train, binary_data.y_train, axs[0])\n",
    "tests.show_decision_surface(knn, binary_data.X_valid, binary_data.y_valid, axs[1])\n",
    "tests.show_decision_surface(knn, binary_data.X_test, binary_data.y_test, axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9204bca4cb30fea1ad06f6e3e29fbfc",
     "grade": false,
     "grade_id": "q15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.5 [5 points] For each k in the range [1,32], fit a KNNClassifier on the training set and plot the accuracies on training and validation\n",
    "data versus k. What's the value of k that yields the best accuracy on the training set? on the validation set? Which one\n",
    " should we choose? (Avg __4.8__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f9fa1a05b6899899e5f6adf55bde49a",
     "grade": true,
     "grade_id": "a15a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 1.5.a\n",
    "#TODO: Try different Ks\n",
    "ks = list(range(1, 32))\n",
    "accuracies_train = []\n",
    "accuracies_valid = []\n",
    "for k in ks:\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "plt.plot(ks, accuracies_valid, label=\"valid\")\n",
    "plt.plot(ks, accuracies_train, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8649f6ef25cc5c9d6f7994f75b3cad7b",
     "grade": true,
     "grade_id": "cell-827f38ab553e6555",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### # Workspace 1.5.b\n",
    "% Write up: best k for training and validation sets, and which one should we choose\n",
    "\n",
    "% YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8f50b305071e3439a7bc3c0e1067281",
     "grade": false,
     "grade_id": "q16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.6 [5 points] Report the accuracy and the confusion matrix on the test set using the value of k chosen in 1.5 (Avg __5__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a99dd0305cb083df0670e965d128e152",
     "grade": true,
     "grade_id": "a16",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 1.6\n",
    "# TODO: print the accuracy and confusion matrix on the test set using k from 1.5\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2155e0bd5112a0a0ca6f488bb7101fb1",
     "grade": false,
     "grade_id": "q17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Bonus (for the avid machine learner) (4 Points)**\n",
    "\n",
    "1.7.a [1 point] A [__consistent classifier__](https://proceedings.neurips.cc/paper/1996/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf)\n",
    "on the training data is defined as a classifier that reaches 100% accuracy on the training set. For which values of k is KNNClassifier Consistent? (Avg __0.5__)\n",
    "\n",
    "1.7.b [2 points] Edit your `KNNClassifier` so that it's consistent for all $k$ (we made sure that the change does not affect the sanity checks) (Avg __1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66f5f2bd5db765b0503f97d99628415d",
     "grade": true,
     "grade_id": "a17a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Write-up for the bonus\n",
    "**Workspace 1.7.a**\n",
    "\n",
    "% for which k in KNNClassifier consistent\n",
    "\n",
    "% YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab36c9e9aa4b984704ef69a1e54e198e",
     "grade": false,
     "grade_id": "digit_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "OK - now we've demonstrated that our KNN classifier works, let's think about our problem space! \n",
    "\n",
    "## Our Dataset - Identifying Digits from Images\n",
    "\n",
    "It's a pretty common problem - just imagine working at the post office, and you're handed a hand-written check,\n",
    "and you have to identify exactly what it says.\n",
    "Did they pay 500 or 600 dollars? Is the letter going to 80309 (campus) or 30309 (Atlanta)?\n",
    "\n",
    "\n",
    "### Problem 2: Improving KNN on Digits dataset - 20 Points\n",
    "\n",
    "2.1 [4 points] `report` the number of examples different partitions of the digit dataset adn the number of pixels in the images (Avg __3.8__)\n",
    "\n",
    "2.2 [6 points] complete the `evaluate` to perform the same evaluation we did in 1.5:\n",
    " - For k in range (1, 20):\n",
    "    - initialize the classifier for k  and train in on the training set\n",
    "    - Compute the accuracy on the validation set and save it\n",
    " - Choose k with the best accuracy on the validation set\n",
    " - Report the accuracy and the confusion matrix on the test set (use `display_confusion` for a cleaner output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d81d34536765981fc2ac54938f9c57b1",
     "grade": true,
     "grade_id": "numbers",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Numbers:\n",
    "    def __init__(self):\n",
    "        self.data = data.DigitData() # it has the same structure as binary_data\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"\n",
    "        Report information about the dataset using the print() function\n",
    "        \"\"\"\n",
    "        # Workspace 2.1\n",
    "        #TODO: Create printouts for reporting the size of each set and the size of each datapoint\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def evaluate(self, classifier_class):\n",
    "        \"\"\"\n",
    "        evaluates instances of the classifier class for different values of k and performs model selection\n",
    "        :param classifier_class: Classifier class (either KNNClassifier or WeightedKNNClassifier)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.2\n",
    "        #BEGIN \n",
    "        # code here (anything between BEGIN and END is yours to edit if needed)\n",
    "        best_valid_k = None\n",
    "        confusion_matrix = None\n",
    "        accuracy = 0\n",
    "        ks = list(range(1, 20))\n",
    "        accuracies_valid = []\n",
    "        for k in ks:\n",
    "            print(k, end=\"\\r\")\n",
    "        #END\n",
    "        print(\"best k:\", best_valid_k)\n",
    "        print(\"Accuracy on test set:\", accuracy)\n",
    "        self.display_confusion(confusion_matrix)\n",
    "\n",
    "    def view_digit(self, index, partition):\n",
    "        \"\"\"\n",
    "        Display a digit given its index and partition\n",
    "        :param index: index of the digit image\n",
    "        :param partition: partition from which the digit is retrieved, either \"train\", \"valid\" or \"test\"\n",
    "        \"\"\"\n",
    "        image = {\"train\": self.data.X_train, \"valid\": self.data.X_valid, \"test\": self.data.X_test}[partition][index]\n",
    "        label = {\"train\": self.data.y_train, \"valid\": self.data.y_valid, \"test\": self.data.y_test}[partition][index]\n",
    "        image = image.reshape(28, 28)\n",
    "        plt.figure()\n",
    "        plt.matshow(image)\n",
    "        plt.title(\"Digit %i\" % label)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def display_confusion(c_matrix):\n",
    "        \"\"\"\n",
    "        Displays the confusion matrix using matshow\n",
    "        :param c_matrix: square confusion matrix, shape (num_classes, num_classes)\n",
    "        \"\"\"\n",
    "        _, ax = plt.subplots()\n",
    "        ax.matshow(c_matrix, cmap=plt.cm.Blues)\n",
    "        for i in range(c_matrix.shape[0]):\n",
    "            for j in range(c_matrix.shape[0]):\n",
    "                ax.text(i, j, str(c_matrix[j, i]), va='center', ha='center')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluate of Numbers and shopws the confusion matrix\n",
    "numbers = Numbers()\n",
    "numbers.report()\n",
    "numbers.evaluate(KNNClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36bfb33cbb231e19174f2e694e95355a",
     "grade": false,
     "grade_id": "q23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Wow, I can't believe we just created a KNN Classifier for multiple classes - but can't we make it better?*\n",
    "\n",
    "Yes, we saw above that our classifier didn't work perfectly. Let's explore try to understand why.\n",
    "\n",
    "\n",
    "2.3 [10 points] Determine which classes are most often confused (from our confusion matrix above),\n",
    " inspect some examples of these digits (using the `view_digit` function in our Numbers class),\n",
    " and write a brief (4 - 5 sentences) description of why you think these particular numbers may be misclassified. (Avg __9.1__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2d664f872f1ac11b24179adec1d4988",
     "grade": true,
     "grade_id": "a23a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 2.3.a\n",
    "#TODO: Print out problem class images\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3b5cbfc682156e6bc12f41693c6ad24",
     "grade": true,
     "grade_id": "a23b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Workspace 2.3.b**\n",
    "\n",
    "TODO: Write description of mis-classification\n",
    "\n",
    "% YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3c1136059067ecd5fae708554ac1a9d",
     "grade": false,
     "grade_id": "wknn",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 3 : Improving KNN on MNIST using WeightedKNN (25 pts)\n",
    "Complete the `WeightedKNNClassifier` class to perform the weighted KNN classification.\n",
    "The Weighted KNN classifier assigns weights to the nearest-neighbor training examples proportional to\n",
    " the inverse-distance from the training example to the query point.\n",
    "\n",
    "Classification is performed by summing the weights associated with each class and predicting the class with the highest weighted-majority vote.\n",
    " Mathematically, we might describe the weighted-vote for a class $c$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Weighted-Vote}(c)(x) = \\sum_{i \\in {\\cal N}_K(x)} I(y_i \\in c) \\times \\frac{1}{\\|{\\bf x}_i - {\\bf x}\\|}\n",
    "\\end{align}\n",
    "\n",
    "where ${\\cal N}_K(x)$ is the set of the nearest $k$ neighbors to $x$.\n",
    "\n",
    "3.1 [10 points] Complete `weighted_vote`: it's certainly possible that a query point could be distance $0$ away from some training example.\n",
    " If this happens your implementation should handle it and return the appropriate class label.(Avg __7.8__)\n",
    "\n",
    "3.2 [5 points] Complete `predict`. (Avg __4.9__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9059abfc0170812c48211eeef1a0da64",
     "grade": true,
     "grade_id": "weighted_knn",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class WeightedKNNClassifier(KNNClassifier):\n",
    "\n",
    "    def weighted_vote(self, indices_nearest_k, distances_nearest_k):\n",
    "        \"\"\"\n",
    "        Given indices of nearest neighbors in training set, return the majority label. \n",
    "        Break ties by considering 1 fewer neighbor until a clear winner is found. \n",
    "\n",
    "        :param indices_nearest_k: The indices of the K nearest neighbors in self.X_train\n",
    "        :param distances_nearest_k: Corresponding distances from query point to K nearest neighbors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 3.1\n",
    "        #BEGIN \n",
    "        labels = np.empty((indices_nearest_k.shape[0]))  #REPLACE\n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an np.array of query points, return y_hat, an np.array of predictions\n",
    "        :param X: an (m x p) dimension np.array of points to predict labels for\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 3.2\n",
    "        ##BEGIN \n",
    "        # code here\n",
    "        labels = None\n",
    "        #END\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# tests.testWeightedKNN(WeightedKNNClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example on how to use the show_decision_surface\n",
    "knn = WeightedKNNClassifier(3).fit(binary_data.X_train, binary_data.y_train)\n",
    "tests.show_decision_surface(knn, binary_data.X, binary_data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71782be4c8ae4e1fa9833f68f404cf6c",
     "grade": false,
     "grade_id": "q33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.3 [5 points] Evaluate `WeightedKNNClassifier` on the binary data, similar to 1.5 digits data using `Numbers` class and compare it to `KNNClassifier` (Avg __4.6__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5391775cc76a04c9967626c9fc731e2",
     "grade": true,
     "grade_id": "a33a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 3.3.a\n",
    "#TODO: Try different Ks\n",
    "ks = list(range(1, 32))\n",
    "accuracies_train = []\n",
    "accuracies_valid = []\n",
    "for k in ks:\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "plt.plot(ks, accuracies_valid, label=\"valid\")\n",
    "plt.plot(ks, accuracies_train, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.max(accuracies_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3b3b67e29d4fb2184850582dd9d92f5",
     "grade": true,
     "grade_id": "a33b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### # Workspace 3.3.b\n",
    "% Write up: Which classifier does better on the binary data\n",
    "\n",
    "% YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85644699c1b9b01c56bb52b5d6744d1a",
     "grade": false,
     "grade_id": "q34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "3.4 [5 points] Compare `WeightedKNNClassifier` to `KNNClassifier` on the digits' data using `Numbers` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60527d8927ebcf41d1af7451b68873e0",
     "grade": true,
     "grade_id": "a34a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Workspace 3.4.a\n",
    "#BEGIN \n",
    "# code here\n",
    "#END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c45ee1fce63b81a3e603e2bbf7265ef0",
     "grade": true,
     "grade_id": "a34b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### 3.4.b write-up\n",
    "% YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4aa162433890a69b7cf15cc29f089cd2",
     "grade": false,
     "grade_id": "q35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Bonus\n",
    "3.5 [2 points] Is `WeightedKNNClassifier` a consistent classifier? why? (Avg __1.1__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cff4b697f30a3e0de454b8463258aab",
     "grade": true,
     "grade_id": "a35",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### 3.5 write-up\n",
    "% YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa3d4cf29da4380132c8112d4408a18d",
     "grade": false,
     "grade_id": "nb_data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 4 - Naive Bayes [25 points]\n",
    "***\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Colorado residency.\n",
    "The dataset looks like the following.\n",
    "\n",
    "|Age|Salary|Colorado Resident| Has Siblings | College degree|\n",
    "|:------:|:-----------:| :----------:| :----------:|--:|\n",
    "| 37 | 44,000 | Yes | No  | Yes|\n",
    "| 61 | 52,000 | Yes | No  | No |\n",
    "| 23 | 44,000 | No  | No  | Yes|\n",
    "| 39 | 38,000 | No  | Yes | Yes|\n",
    "| 48 | 49,000 | No  | No  | Yes|\n",
    "| 57 | 92,000 | No  | Yes | No |\n",
    "| 38 | 41,000 | No  | Yes | Yes|\n",
    "| 27 | 35,000 | Yes | No  | No |\n",
    "| 23 | 26,000 | Yes | No  | No |\n",
    "| 38 | 45,000 | No  | No  | No |\n",
    "| 32 | 50,000 | No  | No  | Yes|\n",
    "| 25 | 52,000 | Yes | No  | Yes|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array([\n",
    "    [37, 44000, 1, 0],\n",
    "    [61, 52000, 1, 0],\n",
    "    [23, 44000, 0, 0],\n",
    "    [39, 38000, 0, 1],\n",
    "    [48, 49000, 0, 0],\n",
    "    [57, 92000, 0, 1],\n",
    "    [38, 41000, 0, 1],\n",
    "    [27, 35000, 1, 0],\n",
    "    [23, 26000, 1, 0],\n",
    "    [38, 45000, 0, 0],\n",
    "    [32, 50000, 0, 0],\n",
    "    [25, 52000, 1, 0]\n",
    "])\n",
    "labels = np.array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6177a313b2b4ee36d1c6232ae15a46ea",
     "grade": false,
     "grade_id": "q41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4.1 [3 points] Complete `threshold_features` to convert age and salary features to binary ones using the threshold arguments. (Avg __2.9__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "035ee116e7cf1b7cd6afbe7474816f62",
     "grade": true,
     "grade_id": "a41",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def threshold_features(features, age_threshold, salary_threshold):\n",
    "    \"\"\"\n",
    "    Transform afe and salary to binary\n",
    "    :param features: data array of shape (m, n_features) where features[:,0] for age, features[:,1} for salary\n",
    "    :param age_threshold: used to \"binarize\" the data, 1 if age > age_threshold and 0 otherwise\n",
    "    :param salary_threshold: used to \"binarize\" the data, 1 if salary > salary_threshold and 0 otherwise\n",
    "    :return: binary features matrix\n",
    "    \"\"\"\n",
    "    binary_features = features * 1  #This row just creates a \"hard copy\" of the X array so we can manipulate it as needed\n",
    "\n",
    "    # Workspace 4.1\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "\n",
    "    return binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# tests.test_threshold(threshold_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4246902ecfc63bb134ab7240177825c1",
     "grade": false,
     "grade_id": "q42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4.2 [2 points] If we were to use only one binary feature (age >40, salary > 40000, colorado resident, has siblings),\n",
    "then what's the highest accuracy we could achieve? Which feature should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40db5aaf6c25d4d279b4ff3dae315021",
     "grade": true,
     "grade_id": "a42",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Workspace 4.2**\n",
    "\n",
    "% YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83cc750ec1e9ad744b791ad4cae9cef3",
     "grade": false,
     "grade_id": "nb_quest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## How to implement NaiveBayes\n",
    "As seen during the class, given a row $(x_1, x_2, x_3)$, the naive Bayes classifier should assign the label $y$ that\n",
    "maximizes:\n",
    "\n",
    "\\begin{align}\n",
    "\\log [p(y) \\prod_i p(x_i | y)] = \\log p(y) + \\sum_{i} \\log p(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "$p(y)$ and $p(x_i | y)$ are computed using the training set (during `fit` call).\n",
    "\n",
    "For this, we need two attributes to store\n",
    "$\\log p(y)$ and $\\log p(x_i | y)$ for different features $i$.\n",
    "\n",
    "Let's assume we're working with binary classes $\\{0, 1\\}$ and all features have discrete supports.\n",
    "Then we will store `classes_log_probability` as an array of shape `(2,)` that contains:\n",
    "\\begin{bmatrix}\n",
    "\\log p(y=0),\\log p(y=1)\n",
    "\\end{bmatrix}.\n",
    "\n",
    "If feature $i$ has $2$ possible values $\\{0, 1\\}$, then $\\log p(x_i | y)$ would be stored as a $ 2 \\times 2$ matrix:\n",
    "\\begin{align}\n",
    "A_i = \\begin{bmatrix}\n",
    "\\log p(x_i=0 | y=0) & \\log p(x_i=1| y=0)\\\\\n",
    "\\log p(x_i=0 | y=1) & \\log p(x_i=1 | y=1)\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "`features_log_likelihood` should then store such matrix for each feature.\n",
    "\n",
    "We have defined $p(x_i | y)$ as :\n",
    "\\begin{align}\n",
    "p(x_i | y) = \\frac{N_{y,x_i}}{N_y}\n",
    "\\end{align}\n",
    "where $N_{y,i}$ is the number of rows where $y$ and $x_i$ occur together, and $N_y = \\sum_i N_{y,x_i}$.\n",
    "\n",
    "4.3 [2 points] Complete the method `compute_classes` and store the log prior in `classes_log_probability` (Avg __2__)\n",
    "\n",
    "4.4 [5 points] Complete the method `compute_features` by storing the matrices $A_i$ in `self.features_log_likelihood` (Avg __4.3__)\n",
    "\n",
    "4.5 [5 points] Complete the method `join_log_likelihood` that computes the likelihood quantities\n",
    " $[\\sum_{i} \\log p(x_i | y=0), \\sum_{i} \\log p(x_i | y=1)]$ for each observation\n",
    "\n",
    "4.6 [3 points] Complete the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a303a298767c0bad4105ace204921fba",
     "grade": true,
     "grade_id": "nb_code",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \"\"\"\n",
    "    NaiveBayes classifier for binary features and binary labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_counts = None\n",
    "        self.classes_log_probability = np.empty((2,))\n",
    "        self.features_log_likelihood = []  # list of arrays where element i store log p(X[:,i], y)\n",
    "\n",
    "    def compute_classes(self, y):\n",
    "        \"\"\"\n",
    "        Computes the log prior of binary classes and stores the result in self.classes_log_probability\n",
    "        :param y: binary labels array, shape (m,)\n",
    "        \"\"\"\n",
    "        # Workspace 4.3\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def compute_features(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the log likelihood matrices for different features and stores them in self.features_log_likelihood\n",
    "        :param X: data matrix with binary features, shape (n_samples, n_features)\n",
    "        :param y: binary labels array, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 4.4\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: binary np.array of shape (n_samples, n_features) [values 0 or 1]\n",
    "        :param y: corresponding binary labels of shape (n_samples,) [values 0 or 1]\n",
    "        :return: Classifier\n",
    "        \"\"\"\n",
    "        self.compute_classes(y)\n",
    "        self.compute_features(X, y)\n",
    "        return self\n",
    "\n",
    "    def joint_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Computes the joint log likelihood\n",
    "        :param X: binary np.array of shape (n_samples, n_features) [values 0 or 1]\n",
    "        :return: joint log likelihood array jll of shape (n_samples, 2), where jll[i] = [log p(X[i]|y=0),log p(X[i]|y=1)]\n",
    "        \"\"\"\n",
    "        # Workspace 4.5\n",
    "        joint_log_likelihood = np.zeros((X.shape[0], 2))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return joint_log_likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 4.6\n",
    "        # TODO: Find the corresponding labels using Naive bayes logic\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        y_hat = np.zeros((X.shape[0],))\n",
    "        #END\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# tests.test_NB(NaiveBayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd3d9885a191c94f68659da177444f0f",
     "grade": false,
     "grade_id": "q47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4.7 [5 points] Using age 40 and salary 40,000 as thresholds, transform the features and evaluate (accuracy) the NaiveBayes classifier\n",
    "on the training data. Does it outperform our baseline (of using one feature)? (Avg __4.5__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f4f4e0863cfcad45224d4c2db59ef94",
     "grade": true,
     "grade_id": "a47",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayes()\n",
    "# Workspace 4.5\n",
    "#TODO: Transform features to binary features, fit the classifier, report the accuracy\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31b1c146204e1873aaf04e7985d3643c",
     "grade": false,
     "grade_id": "q48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bonus**\n",
    "\n",
    "4.8 [2 points] Use the attribute `alpha` of the NaiveBayes to convert it to smoothed NaiveBayes presented during the class.\n",
    "`alpha`defaults to 0, so editing the class should not affect NaiveBayes tests (Avg __1.8__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('v39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a40e90ce5eed5443dbb6a0e58bc3eccaa73bf47dc6240267330c79ebece5075e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
